---
title: "HarvardX - Data Science"
author: "Marco Schicker"
date: "2021"
output: 
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true

---

``` {r setup, include=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, Message=FALSE, fig.width=12, fig.height=8)
#######INSTALL PACKAGES#######
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
if(!require(gam)) install.packages("gam", repos = "http://cran.us.r-project.org")


library(tidyverse)
library(caret)
library(data.table)
library(dplyr)
library(stringr)
library(ggplot2)
library(lubridate)
library(corrplot)
library(rpart)
library(matrixStats)
library(gam)
library(splines)


######## Create edx set, validation set (final hold-out test set) #######

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```

***

# Executive summary

The goal of this project is to predict movie ratings based on a set of predictors. The target is to create a model which predictions minimize the deviations compared with the true rating, i.e. a low root mean square deviation (RMSE).
The means to do so are analyzing a data set provided and training a model to make predictions for a validation set of data. 

The edx-data set provided consists of 9000055 movie ratings. Each rating is categorized by the following variables:  

- userId -- a unique identifier per user
- movieId -- a unique identifier per movie
- title -- The movie title, including the release year of the movie
- timestamp -- The date and time when the movie was rated by the user
- genres -- a list of genres to categorize movies

In general the steps performed to fulfill the task are:   

- importing data
- cleaning data
- exploring and visualizing data
- training different models and comparing performance
- validating the most promising model on the validation set

In the end a Model was found with a RMSE below the target of 0.86490 running on the validation data set. 


# Methods & Analysis
The process to train the models consists of the following basic steps:  

- Data import
- Data exploration, cleaning and visualization
- Data mutation
- Creating training and test set
- Training and comparing different models
- Validation of best model

## Data import

The data used to train the model is derived from the publicly available movielens database. According to provided code a data set "edx" and a dataset "validation" is automatically created. 
The "edx"-data set is used to train and test a model, while the second data set "validation" is used to validate the chosen model. 


## Data exploration, cleaning and visualization

In order to work with the data set it is analyzed and altered.

### Overall analysis
To get a first overview about the data the following table is created:
```{r analysis 1, echo=FALSE}
summary(edx)
```  

The dimensions of the data file are as follows:  
```{r, echo=FALSE} 
dim(edx)
``` 

The data classes are shown in the following table
```{r classes, echo=FALSE}
sapply(edx, class)
```
We can see that the data set "edx" consists of 9000055 rows and six columns with numerical and character variables. There are no NA-Values and on first sight plausible min and max values, so the data quality looks good. 

To get a first inside into the content we take a look at the first rows to see actual real data.

```{r analysis 2 }
head(edx)
```

The column "title" contains the title and the release year of the movie. The columns "timestamp" is formatted as an integer, showing seconds since 1970-01-01. The data will be mutated to include the new columns:  

* "rel_year" - the release year of the movie, derived from the title column
* "ts_year" - year of the timestamp of the rating
* "ts_month" - month of the timestamp of the rating
* "ts_day" - day of the timestamp of the rating

Since integer values occupy only half space compared to numeric the columns with natural numbers will be converted to integer in order to reduce required data size for calculations later on.

The new summary of edx looks like this:
```{r sum_new}
# extract release year from title column to create a new predictor for edx and validation
# extract year, month and day from timestamp
# convert numeric to integer if possible to half data size (8b-->4b) ==> higher modeling speed
edx <- edx %>% 
  mutate(rel_year = str_extract(title, "\\(\\d{4}\\)"), 
         rel_year = as.integer(str_replace_all(rel_year,"\\(|\\)", "")), # remove parantheses and convert to integer
         ts_year = as.integer(year(as_datetime(timestamp))),
         ts_month = as.integer(month(as_datetime(timestamp), label = FALSE)),
         ts_day = as.integer(wday(as_datetime(timestamp), label = FALSE)),
         movieId = as.integer(movieId)
        )

validation <- validation %>%
  mutate(rel_year = str_extract(title, "\\(\\d{4}\\)"), 
         rel_year = as.integer(str_replace_all(rel_year,"\\(|\\)", "")), # remove parantheses and convert to integer
         ts_year = as.integer(year(as_datetime(timestamp))),
         ts_month = as.integer(month(as_datetime(timestamp), label = FALSE)),
         ts_day = as.integer(wday(as_datetime(timestamp), label = FALSE)),
         movieId = as.integer(movieId)
         )

summary(edx)
```


### Find duplicate entries

```{r duplicates, echo=TRUE}
#look for duplicate entries
problems <- which(duplicated(edx))

```

Duplicate entries could pose a problem for any algorithm so we need to check for them and eliminate double entries, if found.
`r length(problems)` duplicate entries were found.

### Find predictors that are highly correlated and remove if necessary

``` {r correlation}
d <- data.frame(userId = edx$userId,
                movieId = edx$movieId,
                rating = edx$rating,
                timestamp = edx$timestamp,
                rel_year = edx$rel_year,
                ts_year = edx$ts_year,
                ts_month = edx$ts_month,
                ts_day = edx$ts_day
                )
corrplot(cor(d), method = "number")
```

As the correlation matrix shows there are only few correlated numeric variables that should be excluded from the analysis. ts_year is highly correlated with timestamp because it is directly derived from the timestamp. That means we can work with only one of the two. To speed up data handling the timestamp will be dropped from the edx and validation dataset. The movieId and the timestamp have a slight correlation which shows, that most ratings happen after and rather close to the release date of a movie. However, movieId and title are highly correlated, as every movie has itÂ´s fixed title. Since we cannot rule out misspelling we will exclude the title and only go with movieId as a predictor. Title will be kept in the data set for check purposes before regularization. 
```{r}
edx <- edx[,-c("timestamp")]
validation <- validation[,-c("timestamp")]
```

### Remove predictors with near zero variation

Next we need to check if we have predictors in our data set that have no or very small variation.We use the function _nearZeroVar_ to find variables that can be omitted.

```{r NZV}
nzv <- nearZeroVar(edx, saveMetrics = TRUE)
nzv[,3:4]
```
As the analysis shows there is no reason to exclude any predictors from the list.


#### Average and distribution  

A first glance at the data shows the distribution of ratings with an overall average rating of `r mean(edx$rating)`.
To get a better grip on distributions of the target variable and the predictors we will have a look at histograms as well as on rating distributions for each predictor individually.

##### Histograms  

The following graphs show number of occurrences of ratings based on various variables.  

```{r hist}
##histogram rating overall
edx %>% ggplot(aes(rating))+
  geom_histogram(binwidth = 0.5)+
  labs(title="Ratings overall")

##histogram rating_year
edx %>% ggplot(aes(ts_year))+
  geom_histogram(binwidth = 1)+
  labs(title="Timestamp Years overall")

##histogram rating_month
edx %>% ggplot(aes(ts_month))+
  geom_histogram(binwidth = 1)+
  labs(title="Timestamp Months overall")

##histogram rating_day
edx %>% ggplot(aes(ts_day))+
  geom_histogram(binwidth = 1)+
  labs(title="Timestamp Weekdays overall")

##histogram release year movies
edx %>% ggplot(aes(rel_year))+
  geom_histogram(binwidth = 1)+
  labs(title="Release Year overall")

##histogram movieId
edx %>% ggplot(aes(movieId))+
  geom_histogram(binwidth = 1)+
  labs(title="MovieId")+
  ylim(0,15000)

##histogram userId
edx %>% ggplot(aes(userId))+
  geom_histogram(binwidth = 1)+
  labs(title="UserId")+
  ylim(0,2500)


```
We can see from the histograms that full ratings are much higher in number compared to half-point ratings. Ratings are within a range of 0.5 and 5 points. A rating of 4 has been given more often compared to any other rating and the distribution has a left tail.
  
The histogram of _timestamp_years_ shows that a few years are very low on ratings (1995, 1998, 2009). On the other hand the years 1996, 2000 and 2005 show a very high amount of ratings. This information will be regarded later after more thorough analysis when it comes to regularization.

The histogram of _timestamp_months_ shows a rather evenly distributed amount of ratings throughout the year with a light drop in late summer and a rise during october until december.

The histogram of _timestamp_days_ also shows a widely evenly distribution with a high on tuesday and wednesday and a low on sunday.

The histogram of _release_year_ shows a very distinctive distribution with most of the rated movies bein released around 1995. This information will come in handy when we come to regularization of the predictors.

the two histograms of _movieId_ and _userId_ both show a wide range of occurances. That seems plausible since there are movies and users who have a high amount of ratings and those with very few ratings. In connection with the avg ratings per user and movie this will most likely become relevant when performing regularization.


##### Smoothplots for single predictors  

To dive deeper into the data graphs are created that show the average, the median and the standard error over each predictor. In addition to this a smoothplot is added to get a first idea about fitting based on single predictors using LOESS as a fitting algorithm. for movieId and userId the smoothplot is omitted since it makes little sense for the independent consecutive numeric values.

```{r smoothplots, message=FALSE}
##plot ratings over timestamp_year (median, avg, se & smoothline)

edx %>% group_by(ts_year)%>%
  summarize(
    n=n(),
    avg=mean(rating),
    med=median(rating),
    se= sd(rating)
  )%>%
  ggplot(aes(x=ts_year, y=avg, ymin=avg-se, ymax=avg+se, col="avg"))+
  geom_point()+
  geom_point(aes(y=med, col="median"))+
  geom_errorbar(aes(alpha=0.3), show.legend = FALSE)+
  geom_smooth()+
  labs(title="Timestamp Year", y="Rating")+
  ylim(0,5)

##deep dive ts_year because of non-conclusive data in 1995
length(which(edx$ts_year==1995))

##plot ratings over timestamp_month (median, avg, se & smoothline)
edx %>% group_by(ts_month)%>%
  summarize(
    n=n(),
    avg=mean(rating),
    med=median(rating),
    se= sd(rating)
  )%>%
  ggplot(aes(x=ts_month, y=avg, ymin=avg-se, ymax=avg+se, col="avg"))+
  geom_point()+
  geom_point(aes(y=med, col="median"))+
  geom_errorbar(aes(alpha=0.3), show.legend = FALSE)+
  geom_smooth()+
  labs(title="Timestamp Month", y="Rating")+
  ylim(0,5)

##plot ratings over timestamp_weekday (median, avg, se & smoothline)
edx %>% group_by(ts_day)%>%
  summarize(
    n=n(),
    avg=mean(rating),
    med=median(rating),
    se= sd(rating)
  )%>%
  ggplot(aes(x=ts_day, y=avg, ymin=avg-se, ymax=avg+se, col="avg"))+
  geom_point()+
  geom_point(aes(y=med, col="median"))+
  geom_errorbar(aes(alpha=0.3), show.legend = FALSE)+
  geom_smooth()+
  labs(title="Timestamp Day", y="Rating")+
  ylim(0,5)

##Dropping non-differentiating variables ts_month and ts_day
edx <- edx[,1:7]
validation <- validation[,1:7]


##plot ratings over release year of the movie (median, avg, se & smoothline)
edx %>% group_by(rel_year)%>%
  summarize(
    n=n(),
    avg=mean(rating),
    med=median(rating),
    se= sd(rating)
  )%>%
  ggplot(aes(x=rel_year, y=avg, ymin=avg-se, ymax=avg+se, col="avg"))+
  geom_point()+
  geom_point(aes(y=med, col="median"))+
  geom_errorbar(aes(alpha=0.3), show.legend = FALSE)+
  geom_smooth()+
  labs(title="Release Year", y="Rating")+
  ylim(0,5)

##plot ratings over genre w/n>1000 ratings (median, avg, se)
edx %>% group_by(genres)%>%
  summarize(
    n=n(),
    avg=mean(rating),
    med=median(rating),
    se= sd(rating)
  )%>%
  arrange(avg)%>%
  filter(n>1000)%>%
  ggplot(aes(x=genres, y=avg, ymin=avg-se, ymax=avg+se, col="avg"))+
  geom_point()+
  geom_point(aes(y=med, col="median"))+
  geom_errorbar(aes(alpha=0.3), show.legend = FALSE)+
  labs(title="Genres > 1000 occurances", y="Rating", x="")+
  ylim(0,5)

##plot ratings over movieId (median, avg)
edx %>% group_by(movieId)%>%
  summarize(
    avg=mean(rating),
    med=median(rating)
  )%>%
  ggplot(aes(x=movieId, y=avg, col="avg"))+
  geom_point()+
  geom_point(aes(x=movieId, y=med, col="median"))+
  labs(title="MovieId", y="Rating", x="Movie Id")+
  ylim(0,5)

##plot ratings over userId (median, avg)
edx %>% group_by(userId)%>%
  summarize(
    avg=mean(rating),
    med=median(rating)
  )%>%
  ggplot()+
  geom_point(aes(x=userId, y=avg, col="avg"))+
  geom_point(aes(x=userId, y=med, col="median"))+
  labs(title="UserId", y="Rating", x="UserId")+
  ylim(0,5)


```



## Insights gained  

As the graphs show there are big differences between the ratings depending on the different predictors. The standard error is comparable for all numeric variables around 1 and varies more widely when we look at genre as a predictor. That shows us that any of these predictors alone 

### timestamp_year  

The average rating doesn't change very much in different days and can be estimated pretty well.We can see that 1995 has no calculated average and standard error. Further analysis shows that we have only 
`r length(which(edx$ts_year==1995))` ratings from 1995 in the data set. The smoothplot shows that this causes the uncertainty to rise and also impacts the RMSE in 1996 negatively. 

### timestamp month and day  
  
We can see from the analyzed data that we have hardly any change of rating or standard error looking at the predictors  

- ts_month
- ts_day  

As we have seen in the histogram there is also hardly change in numbers, so these two variables will be dumped from the edx and validation data set.
```{r drop day and month}
##Dropping non-important variables ts_month and ts_day
edx <- edx[,1:7]
validation <- validation[,1:7]
```

### release year of movie  

The data provides a very useful insight to the predictor. We can see that avg and standard error change significantly based on the release year of the movie. Further we notice a significant drop in avg and median rating after 1980. Compared with the information seen in the istogram we can see, that obviously movies with an early release date receive less ratings but on average better ratings. release year seems to be a very good predictor to differentiate ratings of movies. However, the smoothcurve shows that we have quite a few outliers which shows that we need additional predictors to reach the desired RMSE.

### Genres  

To get a better overview we limit the graph to genres with more than 1000 occurrences. Looking at the results we can see that there are significant differences between different genres. Using genres as a predictor would enable us to account for quite some variation in the data set. As it is categorical data we don't see a smoothplot.

### MovieId  

Analyzing  rating over movieId we can see the strongest spread between avg and median ratings. We can see the full range between 0.5 and 5 in the median range. The median values in between (e.g. 4.75) are created when there is an even number of ratings per movieId and the two middle values are different. The median() function returns the avg of those two values. Using the movieId as a predictor should account for most of the variation. 

### UserId  

The userId is also showing a lot of variation. Similar to MovieId as a predictor we can observe the full range of possible ratings. Next to MovieId this should be the second predictor to start with.

### Concluding insights  

To create the models we will use the five predictors  

1. movieId
2. userId
3. genres
4. rel_year (release year of the movie)
5. ts_year (year of the rating)



## Modeling approach

In order to find the best performing model we will use the predictors as described in the prior chapter individually or in combination and apply different methods to train the models.

### Initialization of modelling  

The data set edx is split into a training set (90% of the data) and a test set (10% of the data). 
```{r modelling data setup}
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
edx_train <- edx[-test_index,]
edx_test <- edx[test_index,]

edx_test <- edx_test %>% 
  semi_join(edx_train, by = "movieId") %>%
  semi_join(edx_train, by = "userId")

```
Secondly the target function is defined. As we are aiming to minimize the rooted mean square error (RMSE) the function is defined as follows:
```{r rmse, echo=TRUE}
#DEFINE RMSE-FUNCTION
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```
The last two intitialisation procedures include calculating the mean rating (as it is used for quite a few of the model calculations) and setting up a dataframe to compare the results of the different models.

```{r, echo=TRUE}
#Calculate Mu (average)
mu <- mean(edx_train$rating) 

#set up results dataframe
rmse_results <- data.frame(method = character(),
                           RMSE = numeric())
```


### List of prediction models  

The models used are described by the following code: 

Model 1 - Average  
Model 2 - Linear Models  
  Model 2.1 - Models with 1 predictor  
    Model 2.1.1 - movieId  
    Model 2.1.2 - userId  
  Model 2.2 - Models with 2 predictors  
    Model 2.2.1 - movieId + userId  
    Model 2.2.2 - movieId + ts_year  
    Model 2.2.3 - userId + genres  
    Model 2.2.4 - userId + rel_year  
  Model 2.3 - Models with 3 predictors  
    Model 2.3.1 - movieId + userId + genres  
    Model 2.3.2 - movieId + userId + rel_year  
    Model 2.3.3 - movieId + userId + ts_year  
  Model 2.4 - Models with 4 predictors  
    Model 2.4.1 - movieId + userId + genres + rel_year  
    Model 2.4.2 - movieId + userId + genres + ts_year  
  Model 2.5 - Model with 5 predictors   
  Model 2.6 - Top performing linear model with regularized predictors  
Model 3 - knn  
Model 4 - decision tree  
Model 5 - loess  
Model 6 - random forest  
Model 7 - GLM  

Some of the models didn't run due to computing power constraints. Those models are marked as _"not able to run"_ and are not included in the comparison of best performing model. However, they are kept in the overview and code, ready to run for anybody with a more potent computer.

### Model 1 - Average  

The average is the starting model and is the basis that we always need to beat. 
It results in a RMSE of 
```{r M_avg}
####MODEL 1 - AVERAGE####
avg_rmse <- RMSE(edx_test$rating, mu)


# add results to dataframe to compare performance of models#
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="1 - Average",
                                     RMSE = avg_rmse ))
rmse_results %>% knitr::kable()
```


### Model 2 - Linear models  

The group of linear models is the biggest in this project and is divided in groups defined by the number of used predictors. The best performing model is finally optimized by using regularization. 
In general the models always work by analyzing the difference between the avg rating of a certain predictor and the overall avg rating. This value is then used to calculate the prediction by adding it to the mean. 

#### Linear models with one predictor  

The two most promising predictors are MovieId and UserId and are compared by running a model based on a single predictor.
```{r M_lin1}
####MODEL 2.1.1 - LM with movieId####

set.seed(1, sample.kind = "Rounding")

movie_avgs <- edx_train %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

predicted_ratings <- mu + edx_test %>% 
  left_join(movie_avgs, by='movieId') %>%
  .$b_i

predicted_ratings <- ifelse(!is.na(predicted_ratings),predicted_ratings, mu)

model_2.1.1_rmse <- RMSE(edx_test$rating, predicted_ratings)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="2.1.1 - linear movieId",
                                     RMSE = model_2.1.1_rmse ))
####MODEL 2.1.2 - LM with userId####

set.seed(1, sample.kind = "Rounding")

user_avgs <- edx_train %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating-mu))

predicted_ratings <- mu + edx_test %>% 
  left_join(user_avgs, by='userId') %>%
  .$b_u

predicted_ratings <- ifelse(!is.na(predicted_ratings),predicted_ratings, mu)

model_2.1.2_rmse <- RMSE(edx_test$rating, predicted_ratings)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="2.1.2 - linear userId",
                                     RMSE = model_2.1.2_rmse ))
rmse_results %>% knitr::kable()
```
As we can see movieId has a slightly lower RMSE and is therefore the better predictor.

#### Linear models with two predictors  

In this group of predictors we follow the same approach but use two predictors. The predictor mix is always chosen in a way that either movieId or userId are the first one and will be paired with one of the remaining other three predictors.
Recalling certain correlations it doesn't make sense to try out every possible combination as for example release year and movieId are correlated.
```{r M_lin2}
####MODEL 2.2.1 - LM with movieId + userId####

set.seed(1, sample.kind = "Rounding")

user_avgs <- edx_train %>% 
  left_join(movie_avgs, by='movieId')%>%
  group_by(userId) %>% 
  summarize(b_u = mean(rating-mu-b_i))


predicted_ratings <- edx_test %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

predicted_ratings <- ifelse(!is.na(predicted_ratings),predicted_ratings, mu)

model_2.2.1_rmse <- RMSE(edx_test$rating, predicted_ratings)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="2.2.1 - linear movieId + userId",
                                     RMSE = model_2.2.1_rmse ))


####MODEL 2.2.2 - LM with movieId + ts_year####

set.seed(1, sample.kind = "Rounding")

movie_avgs <- edx_train %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

ts_year_avgs <- edx_train %>%
  left_join(movie_avgs, by='movieId')%>%
  group_by(ts_year)%>%
  summarize(b_tsy = mean(rating-mu-b_i))

predicted_ratings <- edx_test %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(ts_year_avgs, by='ts_year') %>%
  mutate(pred = mu + b_i + b_tsy) %>%
  .$pred

predicted_ratings <- ifelse(!is.na(predicted_ratings),predicted_ratings, mu+b_i)

model_2.2.2_rmse <- RMSE(edx_test$rating, predicted_ratings)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="2.2.2 - linear movieId + ts_year",
                                     RMSE = model_2.2.2_rmse ))


####MODEL 2.2.3 - LM with userId + genres####

set.seed(1, sample.kind = "Rounding")

genres_avgs <- edx_train %>% 
  left_join(user_avgs, by='userId') %>%
  group_by(genres) %>%
  summarize(b_gen = mean(rating - mu - b_u)) 

predicted_ratings <- edx_test %>% 
  left_join(genres_avgs, by='genres') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_gen + b_u) %>%
  .$pred

predicted_ratings <- ifelse(!is.na(predicted_ratings),predicted_ratings, mu+b_i)

model_2.2.3_rmse <- RMSE(edx_test$rating, predicted_ratings)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="2.2.3 - linear userId + genres",
                                     RMSE = model_2.2.3_rmse ))


####MODEL 2.2.4 - LM with userId + rel_year####

set.seed(1, sample.kind = "Rounding")

rely_avgs <- edx_train %>% 
  left_join(user_avgs, by='userId') %>%
  group_by(rel_year) %>%
  summarize(b_rely = mean(rating - mu - b_u)) 

predicted_ratings <- edx_test %>% 
  left_join(rely_avgs, by='rel_year') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_rely + b_u) %>%
  .$pred

predicted_ratings <- ifelse(!is.na(predicted_ratings),predicted_ratings, mu+b_i)

model_2.2.4_rmse <- RMSE(edx_test$rating, predicted_ratings)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="2.2.4 - linear userId + rel_year",
                                     RMSE = model_2.2.4_rmse ))
rmse_results %>% knitr::kable()

```
The results show that the model with the lowest RMSE is the combination of movieId and userId (2.2.1) and is already beating the target of 0.86490. This combination will also be the basis for all next models.


#### Linear models with three predictors  

The two predictors movieId and userId are combined with the genre in the first model and with the release year in the second model.
```{r M_lin3}
####MODEL 2.3.1 - LM with userId + movieId + genre

tmp <- gc() #garbage collection to free memory

set.seed(1, sample.kind = "Rounding")

genre_avgs <- edx_train %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu - b_i - b_u)) 

pred_ratings <- edx_test %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genre_avgs, by='genres')%>%
  mutate(pred = mu + b_i + b_u + b_g) %>%
  .$pred

pred_ratings <- ifelse(!is.na(pred_ratings),pred_ratings, mu+b_i+b_u)

model_2.3.1_rmse <- RMSE(edx_test$rating, pred_ratings)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="2.3.1 - linear movieId + userId + genre",
                                     RMSE = model_2.3.1_rmse ))

####MODEL 2.3.2 - LM with userId + movieId + rel_year

tmp <- gc() #garbage collection to free memory

set.seed(1, sample.kind = "Rounding")

rely_avgs <- edx_train %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(rel_year) %>%
  summarize(b_g = mean(rating - mu - b_i - b_u)) 

pred_ratings <- edx_test %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(rely_avgs, by='rel_year')%>%
  mutate(pred = mu + b_i + b_u + b_g) %>%
  .$pred

pred_ratings <- ifelse(!is.na(pred_ratings),pred_ratings, mu+b_i+b_u)

model_2.3.2_rmse <- RMSE(edx_test$rating, pred_ratings)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="2.3.2 - linear movieId + userId + rel_year",
                                     RMSE = model_2.3.2_rmse ))

####MODEL 2.3.3 - LM with userId + movieId + ts_year

tmp <- gc() #garbage collection to free memory

set.seed(1, sample.kind = "Rounding")

ts_year_avgs <- edx_train %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(ts_year) %>%
  summarize(b_tsy = mean(rating - mu - b_i - b_u)) 

pred_ratings <- edx_test %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(ts_year_avgs, by='ts_year')%>%
  mutate(pred = mu + b_i + b_u + b_tsy) %>%
  .$pred

pred_ratings <- ifelse(!is.na(pred_ratings),pred_ratings, mu+b_i+b_u)

model_2.3.3_rmse <- RMSE(edx_test$rating, pred_ratings)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="2.3.3 - linear movieId + userId + rel_year",
                                     RMSE = model_2.3.3_rmse ))
rmse_results %>% knitr::kable()

```
The results show that the best performing model is now the one with movieId, userId and genre as predictors (2.3.1). It seems that genre accounts for more separate uncertainty compared to release year. The reason could be that release year and movieId are correlated to some degree and account for the same uncertainty. Both models beat the target on the test set.


#### Linear models with four predictors  

The best performing model with three predictors is complemented with rel_year and ts_year and the final performance is shown in the following overview:
```{r M_lin4}
####MODEL 2.4.1 - LM with movieId + userId + genre + rel_year

tmp <- gc() #garbage collection to free memory

set.seed(1, sample.kind = "Rounding")

rel_year_avgs <- edx_train %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genre_avgs, by='genres')%>%
group_by(rel_year) %>%
  summarize(b_ry = mean(rating - mu - b_i - b_u - b_g)) 

pred_ratings <- edx_test %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genre_avgs, by='genres')%>%
  left_join(rel_year_avgs, by='rel_year')%>%
mutate(pred = mu + b_i + b_u + b_g + b_ry) %>%
  .$pred

pred_ratings <- ifelse(!is.na(pred_ratings),pred_ratings, mu+b_i+b_u+b_g)

model_2.4.1_rmse <- RMSE(edx_test$rating, pred_ratings)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="2.4.1 - linear movieId + userId + genre + rel_year",
                                     RMSE = model_2.4.1_rmse ))


####Model 2.4.2 - movieId + userId + genres + ts_year####

tmp <- gc() #garbage collection to free memory

set.seed(1, sample.kind = "Rounding")

ts_year_avgs <- edx_train %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genre_avgs, by='genres')%>%
  group_by(ts_year) %>%
  summarize(b_tsy = mean(rating - mu - b_i - b_u - b_g)) 

pred_ratings <- edx_test %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genre_avgs, by='genres')%>%
  left_join(ts_year_avgs, by='ts_year')%>%
  mutate(pred = mu + b_i + b_u + b_g + b_tsy) %>%
  .$pred

pred_ratings <- ifelse(!is.na(pred_ratings),pred_ratings, mu+b_i+b_u+b_g)

model_2.4.2_rmse <- RMSE(edx_test$rating, pred_ratings)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="2.4.2 - linear movieId + userId + genre + ts_year",
                                     RMSE = model_2.4.2_rmse ))
rmse_results %>% knitr::kable()

```
As we see the best performing model so far is 2.4.1 - using movieId + userId + genre + rel_year as predictors. This model will be regularized to account for the different weight of adjustments based on the number of ratings.

#### Linear models with five predictors  

_"not able to run"_

#### Regularized linear model  

In order to regularize the impact of the four predictors we will need to penalize predictor values with very few occurrences. In order to do so we define a value lambda that leads to an overall minimized RMSE. That means minimizing the following formula:  


$\frac{1}{N}\sum_{u, i, g, ry}(y_{u, i, g, ry}-\mu-b_i-b_u-b_g-b_{ry})^2 + \lambda(\sum_i b_{i}^2 + \sum_u b_{u}^2 + \sum_g b_{g}^2 + \sum_{ry} b_{ry}^2)$
  
  
The following graph shows the lambda that minimizes the RMSE and the corresponding RMSE in the comparison table.

```{r M_lin_reg}
tmp <- gc()

set.seed(1, sample.kind = "Rounding")

lambdas <- seq(1, 5, 0.25)
rmses <- sapply(lambdas, function(l){
  mu <- mean(edx_train$rating)
  b_i <- edx_train %>% #penalize movieId with few n()
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  b_u <- edx_train %>% #penalize userId with few n()
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  b_g <- edx_train %>% ###penalize genres with few n()
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    group_by(genres)%>%
    summarize(b_g = sum(rating-b_i-b_u-mu)/(n()+l))
  b_ry <- edx_train %>% ###penalize rel_years with few n()
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    left_join(b_g, by='genres')%>%
    group_by(rel_year) %>%
    summarize(b_ry = mean(rating - mu - b_i - b_u - b_g)/(n()+l))
  
  predicted_ratings <- 
    edx_test %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "genres") %>%
    left_join(b_ry, by ="rel_year")%>%
    mutate(pred = mu + b_i + b_u + b_g + b_ry) %>%
    .$pred
  return(RMSE(edx_test$rating, predicted_ratings))
})

qplot(lambdas, rmses)  

lambda <- lambdas[which.min(rmses)]

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="2.4.1 (Regularized) - movie+user+genre+rel_year",  
                                     RMSE = min(rmses)))
rmse_results %>% knitr::kable()
```

As you can see from the graph the minimum RMSE is reached with a lambda of `r lambda`

### Model 3 - knn  

 _"not able to run"_ 
 
### Model 4 - decision tree  

_"not able to run"_

### Model 5 - loess  

The local weighted regression (loess model) is being used on the two predictors _movieId_ and _userId_, takes a long time to run with results not much better compared to the overall rating average. 
```{r M_loess}
####MODEL 5 - LOESS#### 

train_loess <- train(rating ~userId+movieId, 
                     method = "gamLoess", 
                     data = edx_train)
model5_predict <- predict(train_loess, edx_test)

model_5_rmse <- RMSE(edx_test$rating, model5_predict)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="5 - gamLOESS",
                                     RMSE = model_5_rmse ))
rmse_results %>% knitr::kable()

```

### Model 6 - random forest  

_"not able to run"_

### Model 7 - GLM  

As a last model for comparison the generalized linear model is used. Due to computing power we use a model with only two predictors (_movieId_ and _userId_). As expected the results are not as good as the ones from a regularized linear model with four predictors.

```{r M_glm, message=FALSE}
####MODEL 7 - GLM####

train_glm <- train(rating ~userId+movieId, 
                   method = "glm", 
                   data = edx_train)
train_glm$bestTune
model7_predict <- predict(train_glm, edx_test)


model_7_rmse <- RMSE(edx_test$rating, model7_predict)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="7 - GLM",
                                     RMSE = model_7_rmse ))
rmse_results %>% knitr::kable()
```




# Results
Following the approach mentioned above we can see that we reach the best result by running a linear regularized model with the 4 predictors _movieId_, _userId_, _genres_, _rel_year_
This model is validated on the validation data set. As lambda we use the evaluated value of 4.75. As there are very few rows that didn't show up in the training data the prediction function creates a few NA-values that can be identified in the summary. A check shows the titles of those few movies. 

```{r results}
#########VALIDATE BEST MODEL#########


l <- 4.75 #value based on the cross-validation of model 1.4.1
  b_i <- edx_train %>% #penalize movieId with few n()
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  b_u <- edx_train %>% #penalize userId with few n()
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  b_g <- edx_train %>% ###penalize genres with few n()
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    group_by(genres)%>%
    summarize(b_g = sum(rating-b_i-b_u-mu)/(n()+l))
  b_ry <- edx_train %>% ###penalize rel_years with few n()
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    left_join(b_g, by='genres')%>%
    group_by(rel_year) %>%
    summarize(b_ry = mean(rating - mu - b_i - b_u - b_g)/(n()+l))
 
  # final prediction 
  final_predict <- 
    validation %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "genres") %>%
    left_join(b_ry, by ="rel_year")%>%
    mutate(pred = mu + b_i + b_u + b_g + b_ry) %>%
    .$pred

summary(final_predict) #look for NAÂ´s in the data
```
As we can see there are 4 NAÂ´s in the new prediction vector. We conduct a quick check if the titles of the corresponding movies seem to be some very scarcely watched movies.This seems to be the case, so we will exchange the NA's by the mean rating and proceed to the final RMSE calculation.

```{r finalpred2}

validation %>% filter(is.na(final_predict))%>%.$title #check titles of movies w/NAÂ´s
final_predict <- replace(final_predict, is.na(final_predict), mu) #exchange NAÂ´s with avg rating
  
  final_model_rmse <- RMSE(validation$rating, final_predict)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="VALIDATION - 2.4.1 (Regularized) - movie+user+genre+rel_year",  
                                     RMSE = final_model_rmse))
rmse_results %>% knitr::kable()

```


This result is expected as we have seen in the analyzed data that the rankings vary largely across the predictorÂ´s range of values. 
GLM performs lower because it assumes a linear context for all predictors.
The local weighted regression (loess model) assumes that within very small windows the data is linear resp. parabolic. That allows for an overall non-linear context looking at the whole range of data. However this approach considers data points within a span and weighs them differently. Looking at predictors like _movieId_ or _userId_ this approach would not work very well because the Ids have a rather categorical character and two movies or users with very close Ids have independent ratings.


# Conclusion
To sum up the results of this report it was possible to train an algorithm and reach an RMSE of 
`r tail(rmse_results$RMSE, 1)`.
The best performance was reached by using Model _2.6_ with a regularized linear approach.


The limitations of this report lie in the limited data used and in the approach chosen, with a limited amount of pre-chosen predictors. Secondly some of the methods couldnÂ´t be used due to a limitation of computing power. 
In this analysis we only use a fraction of the possible combinations of predictors and training models. Separate predictors with separate models depending on the distribution and for example using an ensemble of those could also be an option to improve performance.  
Next steps to improve performance would be to use other predictors like director, actors, country of origin, languages available, production cost, movie length, amount of awards, and more...
One could also analyze for trigger words within the title and check if there are correlations with ratings if the title contains words like "Love", "reloaded", "fight" or others.
Another way would also be to increase training data size, or use neural networks to find new predictors using deep learning algorithms. In order to do so we would need to provide a broader data base.