---
title: "HarvardX - Data Science"
author: "Marco Schicker"
date: "2021"
output: pdf_document
---

``` {r setup, include=FALSE, eval=T, echo=F, message=F, warning=F}
knitr::opts_chunk$set(echo = TRUE)
```



## Executive summary

The goal of this project is to predict movie ratings based on a set of predictors. The target is to create a model which predictions minimize the deviations compared with the true rating, i.e. a low root mean square deviation (RMSE).
The means to do so are analyzing a data set provided and training a model to make predictions for a validation set of data. 

The edx-data set provided consists of 9000055 movie ratings. Each rating is categorized by the following variables:
* userID - a unique identifier per user
* movieID - a unique identifier per movie
* title - The movie title, including the release year of the movie
* timestamp - The date and time when the movie was rated by the user
* genres - a list of genres to categorize movies

In general the steps performed to fulfill the task are:
* importing data
* cleaning data
* exploring and visualizing data
* training different models and comparing performance
* validating the most promising model on the validation set

In the end a Model was found with a RMSE of ``` {r final result, echo=false } final_model_rmse <- RMSE(validation$rating, final_predict)``` running on the validation data set. 


## Methods & Analysis
The process to train the models consists of the following basic steps:
* data import
* data exploration, cleaning and visualization
* data mutation
* creating training and test set
* training and comparing different models
* validation of best model

### data import
The data used to train the model is derived from the publicly available movielens database. According to provided code a data set "edx" and a dataset "validation" is automatically created. 
The "edx"-data set is used to train a model, while the second data set "validation" is used to validate the chosen model. 


### data exploration, cleaning and visualization

In order to work with the data set it is analyzed and altered

#### Overall analysis
To get a first overview about the data the following table is created:
```{r analysis 1, echo=false}
summary(edx)
dim(edx)
sapply(edx, class)
```
We can see that the data set edx consists of ```{r rows, echo=false} nrow(edx)``` rows and six columns with numerical and character variables. There are no NA-Values and on first sight plausible min and max values, so the data quality looks good. 
```{r analysis 2, echo=false}
head(edx)
```

The column "title" contains the title and the release year of the movie. The colums "timestamp" is formatted as an integer, showing seconds since 1970-01-01. The data will be mutated to include the new columns:
* "rel_year" - the release year of the movie, derived from the title column
* "ts_year" - year of the timestamp of the rating
* "ts_month" - month of the timestamp of the rating
* "ts_day" - day of the timestamp of the rating

Since integer valus occupy only half space compared to numeric the columns with natural numbers will be converted to integer in order to reduce required data size for calculations later on.

The new summary of edx looks like this:
```{r sum_new, echo=false}
summary(edx)
```


#### Find duplicate entries

Duplicate entries could pose a problem for any algorithm so we need to check for them. 
No duplicate entries could be found.

#### Find predictors that are highly correlated and remove if necessary
``` {r correlation, echo = FALSE}
d <- data.frame(userId = edx$userId,
                movieId = edx$movieId,
                rating = edx$rating,
                timestamp = edx$timestamp,
                rel_year = edx$rel_year,
                ts_year = edx$ts_year,
                ts_month = edx$ts_month,
                ts_day = edx$ts_day
                )
corrplot(cor(d), method = "number")
```
As the correlation matrix shows there are only few correlated numeric variables that should be excluded from the analysis. ts_year is highly correlated with timestamp because it is directly derived from the timestamp. That means we can work with only one of the two. To speed up data handling the timestamp will be dropped from the edx and validation dataset. The movieId and the timestamp have a slight correlation which shows, that most ratings happen after and rather close to the release date of a movie. However, movieId and title are highly correlated, as every movie has it´s fixed title. Since we cannot rule out misspelling we will exclude the title and only go with movieId as a predictor. Title will be kept in the data set for check purposes before regularization. 


#### Remove predictors with near zero variation
Next we need to check if we have predictors in our data set that have no or very small variation. 
```{r NZV, echo=false}
nzv <- nearZeroVar(edx, saveMetrics = TRUE)
nzv[,3:4]
```
As the analysis shows there is no reason to exclude any predictors from the list.


#### Average and distribution
A first glance at the data shows the distribution of ratings with an overall average rating of ```{r mean, echo=false} mean(edx$rating)```.
To get a better grip on distributions of the target variable and the predictors we will have a look at histograms as well as on rating distributions for each predictor individually.

##### Histograms  


```{r hist, echo = false}
##histogram rating overall
edx %>% ggplot(aes(rating))+
  geom_histogram(binwidth = 0.5)+
  labs(title="Ratings overall")

##histogram rating_year
edx %>% ggplot(aes(ts_year))+
  geom_histogram(binwidth = 1)+
  labs(title="Timestamp Years overall")

##histogram rating_month
edx %>% ggplot(aes(ts_month))+
  geom_histogram(binwidth = 1)+
  labs(title="Timestamp Months overall")

##histogram rating_day
edx %>% ggplot(aes(ts_day))+
  geom_histogram(binwidth = 1)+
  labs(title="Timestamp Weekdays overall")

##histogram release year movies
edx %>% ggplot(aes(rel_year))+
  geom_histogram(binwidth = 1)+
  labs(title="Release Year overall")

```
We can see from the histograms that full ratings are much higher in number compared to half-point ratings. Ratings are within a range of 0.5 and 5 points. A rating of 4 has been given more often compared to any other rating and the distribution has a left tail. 
The histogram of _timestamp_years_ shows that a few years are very low on ratings (1995, 1998, 2009). On the other hand the years 1996, 2000 and 2005 show a very high amount of ratings. This information will be regarded later after more thorough analysis when it comes to regularization.

The histogram of _timestamp_months_ shows a rather evenly distributed amount of ratings throughout the year with a light drop in late summer and a rise during october until december.

The histogram of _timestamp_days_ also shows a widely evenly distribution with a high on tuesday and wednesday and a low on sunday.

The histogram of _release_year_ shows a very distinctive distribution with most of the rated movies bein released around 1995. This information will come in handy when we come to regularization of the predictors.



##### Smoothplots for single predictors

To dive deeper into the data graphs are created that show the average, the median and the standard error over each predictor. In addition to this a smoothplot is added to get a first idea about fitting based on single predictors using LOESS as a fitting algorithm.

```{r smoothplots, echo=false}
##plot ratings over timestamp_year (median, avg, se & smoothline)

edx %>% group_by(ts_year)%>%
  summarize(
    n=n(),
    avg=mean(rating),
    med=median(rating),
    se= sd(rating)
  )%>%
  ggplot(aes(x=ts_year, y=avg, ymin=avg-se, ymax=avg+se, col="avg"))+
  geom_point()+
  geom_point(aes(y=med, col="median"))+
  geom_errorbar(aes(alpha=0.3))+
  geom_smooth()+
  labs(title="Timestamp Year", y="Rating")+
  ylim(0,5)


##plot ratings over timestamp_month (median, avg, se & smoothline)
edx %>% group_by(ts_month)%>%
  summarize(
    n=n(),
    avg=mean(rating),
    med=median(rating),
    se= sd(rating)
  )%>%
  ggplot(aes(x=ts_month, y=avg, ymin=avg-se, ymax=avg+se, col="avg"))+
  geom_point()+
  geom_point(aes(y=med, col="median"))+
  geom_errorbar(aes(alpha=0.3))+
  geom_smooth()+
  labs(title="Timestamp Month", y="Rating")+
  ylim(0,5)

##plot ratings over timestamp_weekday (median, avg, se & smoothline)
edx %>% group_by(ts_day)%>%
  summarize(
    n=n(),
    avg=mean(rating),
    med=median(rating),
    se= sd(rating)
  )%>%
  ggplot(aes(x=ts_day, y=avg, ymin=avg-se, ymax=avg+se, col="avg"))+
  geom_point()+
  geom_point(aes(y=med, col="median"))+
  geom_errorbar(aes(alpha=0.3))+
  geom_smooth()+
  labs(title="Timestamp Day", y="Rating")+
  ylim(0,5)

##plot ratings over release year of the movie (median, avg, se & smoothline)
edx %>% group_by(rel_year)%>%
  summarize(
    n=n(),
    avg=mean(rating),
    med=median(rating),
    se= sd(rating)
  )%>%
  ggplot(aes(x=rel_year, y=avg, ymin=avg-se, ymax=avg+se, col="avg"))+
  geom_point()+
  geom_point(aes(y=med, col="median"))+
  geom_errorbar(aes(alpha=0.3))+
  geom_smooth()+
  labs(title="Release Year", y="Rating")+
  ylim(0,5)

##plot ratings over genre w/n>1000 ratings (median, avg, se & smoothline)
edx %>% group_by(genres)%>%
  summarize(
    n=n(),
    avg=mean(rating),
    med=median(rating),
    se= sd(rating)
  )%>%
  filter(n>1000)%>%
  arrange(avg)%>%
  ggplot(aes(x=genres, y=avg, ymin=avg-se, ymax=avg+se, col="avg"))+
  geom_point()+
  geom_point(aes(y=med, col="median"))+
  geom_errorbar(aes(alpha=0.3))+
  labs(title="Genres > 1000 occurances", y="Rating")+
  ylim(0,5)

##plot ratings over movieId (median, avg, se & smoothline)
edx %>% group_by(movieId)%>%
  summarize(
    n=n(),
    avg=mean(rating),
    med=median(rating),
    se= sd(rating)
  )%>%
  ggplot(aes(x=movieId, y=avg, ymin=avg-se, ymax=avg+se, col="avg"))+
  geom_point()+
  geom_point(aes(y=med, col="median"))+
  #geom_errorbar(aes(alpha=0.3))+
  labs(title="MovieId", y="Rating", x="Movie Id")+
  ylim(0,5)
```



### Insights gained

As the graphs show there are big differences between the ratings depending on the different predictors. The standard error is comparable for all numeric variables around 1 and varies more widely when we look at genre as a predictor. That shows us that any of these predictors alone 

#### timestamp_year
The average rating doesn't change very much in different days and can be estimated pretty well.We can see that 1995 has no calculated average and standard error. Further analysis shows that we have only 
```{r}
##deep dive ts_year because of non-conclusive data in 1995
length(which(edx$ts_year==1995))
```
ratings from 1995 in the data set. The smoothplot shows that this causes the uncertainty to rise and also impacts the RMSE in 1996 negatively. 

#### timestamp month and day

We can see from the analyzed data that we have hardly any change of rating or standard error looking at the predictors
* ts_month
* ts_day
As we have seen in the histogram there is also hardly change in numbers, so these two variables will be dumped from the edx and validation data set.
```{r drop day and month, echo=false}
##Dropping non-important variables ts_month and ts_day
edx <- edx[,1:7]
validation <- validation[,1:7]
```

#### release year of movie
The data provides a very useful insight to the predictor. We can see that avg and standard error change significantly based on the release year of the movie. Further we notice a significant drop in avg and median rating after 1980. Compared with the information seen in the istogram we can see, that obviously movies with an early release date receive less ratings but on average better ratings. release year seems to be a very good predictor to differentiate ratings of movies. However, the smoothcurve shows that we have quite a few outliers which shows that we need additional predictors to reach the desired RMSE.

#### Genres
To get a better overview we limit the graph to genres with more than 1000 occurances. Looking at the results we can see that there are significant differences between different genres. Using genres as a predictor would enable us to account for quite some variation in the data set. As it is categorical data we don't see a smoothplot.

#### MovieId
Analyzing  rating over MoviId we can see the strongest spread between avg and median ratings. We can see the full scale between 0.5 and 5 in the median range. Using the movieId as a predictor should account for most of the variation. 



To create the models we will use the five remaining predictors
1. movieId
2. userId
3. genres
4. rel_year (release year of the movie)
5. ts_year (year of the rating)



### modeling approach


We will use the predictors as described in the prior chapter. 

#### List of prediction models
We will compare various prediction models and combinations of predictors by their performance. They are described by the following code:

Model 1 - Average
Model 2 - Linear Models
  Model 2.1 - Models with 1 predictor
    Model 2.1.1 - movieId
    Model 2.1.2 - userId
  Model 2.2 - Models with 2 predictors
    Model 2.2.1 - movieId + userId
    Model 2.2.2 - movieId + ts_year
    Model 2.2.3 - userId + genres
    Model 2.2.4 - userId + rel_year
  Model 2.3 - Models with 3 predictors
    Model 2.3.1 - movieId + userId + genres
    Model 2.3.2 - movieId + userId + rel_year
  Model 2.4 - Models with 4 predictors
    Model 2.4.1 - movieId + userId + genres + rel_year
    Model 2.4.2 - movieId + userId + genres + ts_year
  Model 2.5 - Model with 5 predictos 
  Model 2.6 - Top performing linear model with regularized predictors
Model 3 - knn
Model 4 - decision tree
Model 5 - loess
Model 6 - random forest
Model 7 - GLM


## Results
Following the approach mentioned above we can see that we reach the best result by combining the predictor set X (a, b, c, d) with the prediction method Y (ensemble)
```{r echo=false rmse_results %>% knitr::kable()}

````


### comparison of model performance


## Conclusion
To sum up the results of this report it was possible to train an algorithm and reach an _RMSE of _.
The best performance was reached by using Model _2.5.2_ with a regularized :
* 
* 
*


The limitations of this report lie in the limited data used and in the approach chosen, with a limited amount of pre-chosen predictors. Secondly some of the methods couldn´t be used due to a limitation of computing power. 
Next steps to improve performance would be to use other predictors like director, actors, country of origin, languages available, production cost, movie length, amount of awards, and more...
One could also analyze for trigger words within the title and check if there are correlations with ratings if the title contains words like "Love", "reloaded", "fight" or others.
Another way would also be to increase training data size, or use neural networks to find new predictors using deep learning algorithms. In order to do so we would need to provide a broader data base.